{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS MINING НА ТЕКСТОВЫХ ЧАТАХ ПО СДЕЛКАМ С НЕДВИЖИМОСТЬЮ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для совершения сделок с недвижимостью обычно требуется предоставить пакет документов, который зависит от различных факторов. При уточнении списка документов клиенты активно используют текстовые чаты. От эффективности взаимодействия консультантов с клиентами, зависит успешность сделок. Одним из критериев оценки эффективности является минимизация времени, которое тратит клиент на подготовку документов.  Взаимодействие консультантов с клиентами можно представить в виде процесса, в рамках которого последовательно запрашиваются документы. Таких процессов достаточно много, и они не формализованы. Поэтому было необходимо провести анализ чатов, выделить наиболее часто встречаемые процессы и т. д.\n",
    "\n",
    "У каждого чата есть свой уникальный идентификатор, у сообщения – тег отправителя, дата и время создания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для текущей задачи потребовался список возможных документов, которые могут запросить в ходе сделки с недвижимостью. В данном случае перечень документов был получен с помощью кластеризации сообщений от консультантов. Записываем найденные документы в список без предлогов.\n",
    "\n",
    "Перед поиском документов необходимо очистить сообщения от стоп-слов (предлоги, часто употребляемые слова, например, приветствия и т.д.), символов, ссылок и чисел. Также заменить частые аббревиатуры на полные названия, например ДКП – на договор купли-продажи. После фильтрации текста производим токенизацию и стемминг. Список документов преобразовываем в список n-грамм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list_tokens = []\n",
    "for d in document_list:\n",
    "    document_list_tokens.append([Porter.stem(el) for el in d.split()])\n",
    "documents_one_grams = [el for el in document_list_tokens if len(el) == 1]\n",
    "documents_bigrams = [el for el in document_list_tokens if len(el) == 2]\n",
    "documents_3grams = [el for el in document_list_tokens if len(el) == 3]\n",
    "documents_4grams = [el for el in document_list_tokens if len(el) == 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Листинг 1. Преобразование документов в список n-грамм. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью библиотеки nltk токены сообщений также преобразовываем в список n-грамм и сравниваем их с n-граммами документов. На выходе получаем вектор, где значение «1» на конкретной позиции означает, встречался ли документ в сообщении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "def search_docs(tokens):\n",
    "    res_dict = {}\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        if [t] in documents_one_grams:\n",
    "            if res_dict.get(t):\n",
    "                res_dict[t] += 1\n",
    "            else:\n",
    "                res_dict[t] = 1\n",
    "    token_bigrams = ngrams(tokens,2)\n",
    "    for gram in token_bigrams:\n",
    "        if list(gram) in documents_bigrams:\n",
    "            if res_dict.get(gram):\n",
    "                res_dict[gram] += 1\n",
    "            else:\n",
    "                res_dict[gram] = 1\n",
    "    token_3grams = ngrams(tokens,3)\n",
    "    for gram in token_3grams:\n",
    "        if list(gram) in documents_3grams:\n",
    "            if res_dict.get(gram):\n",
    "                res_dict[gram] += 1\n",
    "            else:\n",
    "                res_dict[gram] = 1\n",
    "    token_4grams = ngrams(tokens,4)\n",
    "    for gram in token_4grams:\n",
    "        if list(gram) in documents_4grams:\n",
    "            if res_dict.get(gram):\n",
    "                res_dict[gram] += 1\n",
    "            else:\n",
    "                res_dict[gram] = 1\n",
    "    \n",
    "    for d in document_list_tokens:\n",
    "        val = res_dict.get(tuple(d)) \n",
    "        val_one_gram = None\n",
    "        if len(d) == 1:\n",
    "            val_one_gram = res_dict.get(str(d[0]))\n",
    "        if val is None and val_one_gram is None:\n",
    "            res.append(0)\n",
    "        else:\n",
    "            res.append(1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Листинг 2. Поиск документов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приводим найденные документы в чатах к структуре графа. Для этого для каждого чата необходимо учитывать порядок, в котором были запрошены документы. Учитывая время создания сообщений, формируем ребра графа. Пример создания ребер представлен на картинках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого сообщения выделяем n-граммы документов, которые в нем встречаются, и запоминаем порядок. Далее соединяем каждый документ попарно со следующим по порядку.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как были сформированы все ребра, можно провести анализ: посчитать количество переходов по каждому ребру. Посмотреть часто встречаемые ребра, они демонстрируют типичный процесс, поэтому требуют более детального анализа. Возможно, они показывают эталонный процесс запроса документов, при котором время клиента затрачивается минимально, а возможно, наоборот, показывают распространённые ошибки консультантов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-4.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ребра, которые встречаются редко, являются отклонениями от основного процесса. Возможно, запрашиваются документы, которые в данном виде сделки не нужны. Также необходимо обратить внимание на зеркальные ребра, они показывают, что консультанты часто запрашивают данные документы последовательно через определенное время, хотя можно было запросить их параллельно.\n",
    "\n",
    "В итоге был получен следующий граф процесса:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-5.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный материал показывает, как применять process mining в тех случаях, когда статусы событий определяются с помощью анализа текста. Метод, описанный в статье, подходит и для других задач, например для анализа чатов технической поддержки. Анализируя сообщения в чатах таким образом, можно выявить типичные ошибки и изменить процесс в сторону уменьшения затрачиваемого клиентом времени, оценить работу консультантов.              \n",
    "\n",
    "Весь код можно в этой же ветке"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
